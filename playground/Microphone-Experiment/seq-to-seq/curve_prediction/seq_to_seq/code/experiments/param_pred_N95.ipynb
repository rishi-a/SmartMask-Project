{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Rohit', 1: 'Rishiraj', 3: 'Anubhav', 4: 'Raviraj', 5: 'Maithili', 9: 'Indradeep Mastan', 10: 'Zeel', 12: 'Dhara', 14: 'Jinia', 17: \"Nipun Batra's MIL\", 18: \"Nipun Batra's W\", 19: 'Udit Bhatia', 20: \"Udit Bhatia's W\", 21: 'Aslam', 22: 'Akshat'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_list = [0,1,3,4,5,9,10,12,14,17,18,19,20,21,22]\n",
    "def get_names(file_list):\n",
    "    dc = dict()\n",
    "    df_ground_truth = pd.read_csv(\"files_seq_to_point/PEF/y/y.csv\")\n",
    "    for i in file_list:\n",
    "        dc[i] = df_ground_truth[\"Name\"][i]\n",
    "    return dc\n",
    "\n",
    "dc = get_names(file_list)\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_samples(file1,file2,window_size,normalized=True):\n",
    "\n",
    "    lst1 = pd.read_csv(file1)['Flow'].to_numpy()\n",
    "    lst2 = pd.read_csv(file2)['Flow'].to_numpy()\n",
    "\n",
    "    if normalized:\n",
    "        lst1 = (lst1-min(lst1))/(max(lst1)-min(lst1))\n",
    "        lst2 = (lst2-min(lst2))/(max(lst2)-min(lst2))\n",
    "\n",
    "\n",
    "    min_len = min(len(lst1),len(lst2))\n",
    "    lst_final1 = []\n",
    "    lst_final2 = []\n",
    "    for i in range(min_len-window_size):\n",
    "        lst_final1.append(lst1[i:i+window_size])\n",
    "        lst_final2.append(lst2[i:i+window_size])\n",
    "\n",
    "    lst_final1 = np.array(lst_final1)\n",
    "    lst_final2 = np.array(lst_final2)\n",
    "    return lst_final1,lst_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_seq_to_point(file_list,window_size,param = \"PEF\"):\n",
    "    main_arr = np.empty((0,window_size), int)\n",
    "    df_ground_truth = pd.read_csv(\"files_seq_to_point/{0}/N95/y/y.csv\".format(param))\n",
    "    pef_gt = df_ground_truth[\"PEF\"].to_numpy()\n",
    "    df_filtered = []\n",
    "    for i in file_list:\n",
    "        df = pd.read_csv(\"files_seq_to_point/{1}/N95/X/{0}.csv\".format(i,param))\n",
    "        seq = df[\"Flow\"]\n",
    "#         seq = (seq-min(seq))/(max(seq)-min(seq))\n",
    "        seq = seq.to_list()[:200]\n",
    "#         seq = seq+[df_ground_truth[\"Age\"][i]]*10\n",
    "#         seq = seq + [int(df_ground_truth[\"Sex\"][i]==\"M\")]*10\n",
    "        seq = np.array(seq)\n",
    "        main_arr = np.append(main_arr,np.array([seq]),axis=0)\n",
    "        df_filtered.append(pef_gt[i])\n",
    "       \n",
    "    \n",
    "    main_arr = np.array(main_arr)\n",
    "    main_arr = main_arr.reshape(len(main_arr),window_size,1)\n",
    "    np.save(\"X.npy\",main_arr)\n",
    "    np.save(\"y.npy\",np.array(df_filtered))\n",
    "    \n",
    "\n",
    "# file_list = [0,1,3,4,5,9,10,12,14,17,18,19,20,21,22]\n",
    "# get_dataset_seq_to_point(file_list,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "window_size = 200+0\n",
    "def getmodel(window_size=window_size):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=30, kernel_size=10, activation='relu', strides=1,input_shape=(window_size,1)))\n",
    "    model.add(Conv1D(filters=30, kernel_size=8, activation='relu'))\n",
    "    model.add(Conv1D(filters=40, kernel_size=6, activation='relu'))\n",
    "    model.add(Conv1D(filters=50, kernel_size=5, activation='relu'))\n",
    "    model.add(Conv1D(filters=50, kernel_size=5, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = getmodel()\n",
    "# model.fit(X,y,epochs = 10,batch_size = 32,verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### LEAVE ONE OUT ##################\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def get_LEO_results():\n",
    "    X = np.load(\"X.npy\")\n",
    "    y = np.load(\"y.npy\")\n",
    "    \n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(X)\n",
    "\n",
    "    logs_lst = []\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = getmodel()\n",
    "        history = model.fit(X_train,y_train,epochs = 20,batch_size = 32,verbose=0,validation_data=(X_test,y_test))  \n",
    "        logs_lst.append(history)\n",
    "    \n",
    "    for i in logs_lst:\n",
    "        print(logs_lst.index(i),\" : \", i.history[\"loss\"][-1], i.history[\"val_loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(logs_lst))\n",
    "# logs_lst[0]\n",
    "# from pprint import pprint\n",
    "# pprint(vars(logs_lst[5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] TEST: [0]\n",
      "TRAIN: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14] TEST: [1]\n"
     ]
    }
   ],
   "source": [
    "file_list = [0,1,3,4,5,9,10,12,14,17,18,19,20,21,22]\n",
    "get_dataset_seq_to_point(file_list,200,param = \"PEF\")\n",
    "get_LEO_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] TEST: [0]\n",
      "TRAIN: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14] TEST: [1]\n",
      "TRAIN: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14] TEST: [2]\n",
      "TRAIN: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14] TEST: [3]\n",
      "TRAIN: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14] TEST: [4]\n",
      "TRAIN: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14] TEST: [5]\n",
      "TRAIN: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14] TEST: [6]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14] TEST: [7]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14] TEST: [8]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14] TEST: [9]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14] TEST: [10]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14] TEST: [11]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14] TEST: [12]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14] TEST: [13]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] TEST: [14]\n",
      "0  :  46.41586685180664 60.68410110473633\n",
      "1  :  45.82973098754883 68.88999938964844\n",
      "2  :  48.13597869873047 36.602500915527344\n",
      "3  :  4.094815731048584 1.8766566514968872\n",
      "4  :  4.57673978805542 9.36395263671875\n",
      "5  :  47.667236328125 43.16490173339844\n",
      "6  :  7.44860315322876 4.9602861404418945\n",
      "7  :  6.225438117980957 0.19734248518943787\n",
      "8  :  5.089806079864502 0.14515522122383118\n",
      "9  :  48.9432373046875 25.300901412963867\n",
      "10  :  6.1994147300720215 0.4606582522392273\n",
      "11  :  43.777984619140625 97.6144027709961\n",
      "12  :  4.8240485191345215 1.8687609434127808\n",
      "13  :  7.803122520446777 14.32910442352295\n",
      "14  :  47.329986572265625 47.88640213012695\n"
     ]
    }
   ],
   "source": [
    "file_list = [0,1,3,4,5,9,10,12,14,17,18,19,20,21,22]\n",
    "get_dataset_seq_to_point(file_list,200,param = \"FVC\")\n",
    "get_LEO_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] TEST: [0]\n",
      "TRAIN: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14] TEST: [1]\n",
      "TRAIN: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14] TEST: [2]\n",
      "TRAIN: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14] TEST: [3]\n",
      "TRAIN: [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14] TEST: [4]\n",
      "TRAIN: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14] TEST: [5]\n",
      "TRAIN: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14] TEST: [6]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14] TEST: [7]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14] TEST: [8]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14] TEST: [9]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 14] TEST: [10]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14] TEST: [11]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14] TEST: [12]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14] TEST: [13]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] TEST: [14]\n",
      "0  :  3.197166919708252 3.7622368335723877\n",
      "1  :  6.533069133758545 8.254406929016113\n",
      "2  :  48.13597869873047 36.602500915527344\n",
      "3  :  6.514857769012451 2.700479030609131\n",
      "4  :  49.57304000854492 16.483598709106445\n",
      "5  :  47.667236328125 43.16490173339844\n",
      "6  :  8.346330642700195 5.4563679695129395\n",
      "7  :  2.626708507537842 1.2206699848175049\n",
      "8  :  3.1236910820007324 0.001289640087634325\n",
      "9  :  48.9432373046875 25.300901412963867\n",
      "10  :  6.84562349319458 1.368632435798645\n",
      "11  :  43.777984619140625 97.6144027709961\n",
      "12  :  6.174729347229004 0.652776300907135\n",
      "13  :  46.43809127807617 60.37289810180664\n",
      "14  :  47.329986572265625 47.88640213012695\n"
     ]
    }
   ],
   "source": [
    "file_list = [0,1,3,4,5,9,10,12,14,17,18,19,20,21,22]\n",
    "get_dataset_seq_to_point(file_list,200,param = \"FEV\")\n",
    "get_LEO_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
