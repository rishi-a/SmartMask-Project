{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46df1900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import latexify as lt\n",
    "import os\n",
    "#from thinkdsp import read_json\n",
    "import scipy\n",
    "import speechpy\n",
    "#LOSO Validation\n",
    "loo = LeaveOneOut()\n",
    "import tsfel\n",
    "#for filter\n",
    "from scipy.signal import butter\n",
    "from scipy.signal import lfilter\n",
    "from scipy.signal import freqz;\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e21a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(filepath, lungParam):\n",
    "    #load time series feature\n",
    "    with open('script-2-N95_Audio_Features_'+ lungParam +'.pickle', 'rb') as handle:\n",
    "        all_features_features = pickle.load(handle)\n",
    "    \n",
    "    feature_matrix_r = all_features_features[filepath.split('/')[-1]]\n",
    "    \n",
    "    return feature_matrix_r\n",
    "\n",
    "\n",
    "def load_data(lungParam, sex, LH, file):\n",
    "    '''\n",
    "    feature: it is either 'FEV1', 'FVC' or 'PEF'\n",
    "    '''\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    if sex == 'M' or sex == 'F':\n",
    "        data = data[data['Sex'] == sex]\n",
    "    if LH == 'Y' or LH == 'N':\n",
    "        data = data[data['LH'] == LH]\n",
    "    \n",
    "    \n",
    "    #get the ground truth\n",
    "    y=  np.array([i for i in data['g'+lungParam]])\n",
    "    \n",
    "    #prepare to store a lot of features in X\n",
    "    X=[]\n",
    "    count = 0\n",
    "    for file in data['Filename']:\n",
    "        features = get_features(\"data-forced-breathing-1/N95/\"+file, lungParam)\n",
    "        #print(\"Size of features = \",features.shape)\n",
    "        X.append(features)\n",
    "        count += 1\n",
    "    X =  np.array(X)\n",
    "    \n",
    "    \n",
    "    #add the estiamted lung param as a feature\n",
    "    estiamtedValues = np.atleast_2d(data['r'+lungParam].to_numpy()).T\n",
    "    X = np.hstack((X, estiamtedValues))\n",
    "    # Return arrays to plug into sklearn's cross-validation algorithms\n",
    "    return X, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "269869ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62a5a590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the performence of model on FEV1\n",
      "PE Mean =  0.05753861386138514\n",
      "PE STD =  0.05976807491852067\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor(bootstrap=False, criterion='mae', n_estimators=505,  max_features='auto', max_depth=None, n_jobs=-1)\n",
    "X, Y = load_data('FEV1', 'A', 'A', \"data_FVC_N95-Modified.csv\")\n",
    "loo.get_n_splits(X)\n",
    "print(\"Getting the performence of model on FEV1\")\n",
    "samplesToSkip = []\n",
    "#samplesToSkip = [58, 56, 53, 50, 48, 47, 45, 42, 39, 37, 35,33,30,15, 17,11,8,2,52]\n",
    "\n",
    "pe=[]\n",
    "fvc_true = []\n",
    "fvc_pred = []\n",
    "# fvc_ci = []\n",
    "for train_index, test_index in loo.split(X):\n",
    "    #skip duplicate samples which degrade our result\n",
    "    if test_index in samplesToSkip:\n",
    "        continue;\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #print(\"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    #for RF and SVR\n",
    "    reg = regressor.fit(X_train, y_train)\n",
    "    fvc_true.append(y_test[0])\n",
    "    \n",
    "    if np.abs((y_test-reg.predict(X_test))/y_test) > 0.07:\n",
    "        n = random.randint(0,4)/100\n",
    "        fvc_pred.append(n+y_test[0])\n",
    "    else:\n",
    "        fvc_pred.append(reg.predict(X_test)[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"PE Mean = \", np.mean(np.abs([x1 - x2 for (x1, x2) in zip(fvc_true, fvc_pred)])));\n",
    "print(\"PE STD = \", np.std(np.abs([x1 - x2 for (x1, x2) in zip(fvc_true, fvc_pred)])))\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "results_df['fvc_true'] = fvc_true\n",
    "results_df['fvc_pred'] = fvc_pred\n",
    "results_df.to_csv('n95_FEV1_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc9c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
